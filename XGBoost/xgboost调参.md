#### xgboost调参

- ```import xgboost```
- 通用参数：宏观参数控制
  - booster[默认gbtree]：gbtree、gbliner(很少用)
  - slient[默认0]：参数为1时，静默模式开启，不输出任何信息
  - nthread[默认值为最大可能的线程数]：用来进行多线程控制，应当输入系统的核数。默认最大核数
- Booster参数：控制每一步的booster(gbtree涉及参数)
  - eta[默认0.3]：和GBM中的learning rate 参数类似。通过减少每一步都权重，可以提高模型的鲁棒性。典型值为0.01-0.2
  - min_child_weight[默认1]：决定最小叶子节点样本权重和。和GBMd min_child_leaf参数类似，参数用于**避免过拟合**，当值较大时。，可以避免模型学习到局部的特殊样本。但是如果这个值过高会导致欠拟合。该参数需要使用**CV函数**来调整
  - max_depth[默认6]：这个值为树的最大深度，用来**避免过拟合**，需要使用**CV函数**进行调优。典型值：3-10
  - max_leaf_nodes：树上最大的节点或叶子的数量。可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成${n^2}$个叶子。如果定义了这个参数，GBM会忽略max_depth参数。
  - gamma[默认0]：在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。这个参数的值越大，算法越保守，**该参数与损失函数息息相关，需要调整**。
  - max_delta_step[默认0]：限制每棵树权重改变的最大步长。参数值为0，则没有约束。被赋予某个正值，会让算法更加保守。**通常不需要设置**，**但各类别的样本十分不平衡时，对逻辑回归是很有帮助的**
  - subsample[默认1]：同GBM的subsample参数，该参数控制对于每棵树，随机采样的比例，减少这个值**可以避免过拟合**，典型值：0.5-1
  - colsample_bytree[默认1]：控制每棵随机采样的列数的占比。典型值：0.5-1
  - colsample_bylevel[默认1]：用来控制树的每一级的每一次分裂，对列数随机采样的占比，一般和colsample_bytree作用重合。
  - lambda[默认1]：L2正则化项，在减少过拟合上可以i挖掘更多的用处
  - alpha[默认1]：L1正则化项。在高维情况下，发挥降维的作用，让算法速度更快
  - scle_pos_weight[默认1]：在各类别样本十分不平衡时，该参数设为正值，可以更快的收敛。
- 学习目标参数：控制训练目标的表现
  1. objective[默认reg:linear]：
     1. 该参数定义需要被最小化的损失函数。常用的值有：
        - binary:logistic：二分类逻辑回归，返回预测的概率
        - multi:softmax：使用softmax的多分类器，返回预测的类别
          - 在这种情况下，还需要多设一个参数：
            - num_class(类别数目)
            - multi:softprob和multi:softmax一样，但返回的是每个数据属于各个类别的概率
     2. eval_metric[默认值取决于objective参数的取值]
        - 回归问题：rmse-均方根误差
        - mae：平均绝对误差
        - 分类问题：error-二分类错误率，阈值为0.5
        - logloss：负对数似然函数值
        - merror：多分类错误率
        - auc：曲线下面积
     3. seed[默认0]
        - 随机数的种子，设置之后可以复现随机数据的结果，也可以用于调整参数

#### 参数调优的一般方法：

1. 选择较高的学习速率(learning_rate)，一般情况下，学习速率的值为0.1，对于不同问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量(based_tree)——可以通过CV函数，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。
2. 对于给定学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma,subsample, colsample_bytree)
3. xgboost的**正则化参数的调优**，（lambda, alpha），降低模型复杂度，提高模型的表现
4. 降低学习速率，确定理想参数。

ps：参数调优依赖于高负荷的**栅格搜索**（grid_search）

**特别的：**

1. 仅仅靠参数的调整和模型的小幅优化，想要让模型的表现有个大幅度的提升是不可能的。
2. 要想让模型的表现有一个质的飞跃，需要依靠其他的手段，诸如，特征工程（feature engineering）模型组合（ensemble of model）堆叠（stacking）
3. 全Pipeline优化：HyperGBM